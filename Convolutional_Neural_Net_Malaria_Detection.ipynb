{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPasH1Zp20/FYOGI/WuNHOG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirpouya/TensorFlow-Tutorial/blob/main/Convolutional_Neural_Net_Malaria_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8JwkVBzQ9Wsw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras.layers import Dense, InputLayer, MaxPool2D, Conv2D, Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, dataset_info = tfds.load(\"malaria\", with_info=True, as_supervised=True, shuffle_files=True, split=[\"train\"])"
      ],
      "metadata": {
        "id": "7YZbos7F-cW3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1XCsRbS-p0E",
        "outputId": "c511e1c3-aa79-46a3-e5d8-9e4f8f6282af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<_PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "it's a dictionary"
      ],
      "metadata": {
        "id": "60ArEeatBdjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_RATIO = 0.7\n",
        "VAL_RATIO = 0.2\n",
        "TEST_RATIO = 0.1"
      ],
      "metadata": {
        "id": "D--ao4TlDvzx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splits(dataset, TRAIN_RATIO, VAL_RATIO, TEST_RATIO):\n",
        "  DATASET_SIZE = len(dataset)\n",
        "  # print(f\"data set size: {DATASET_SIZE}\")\n",
        "\n",
        "  train_dataset = dataset.take(int(TRAIN_RATIO * DATASET_SIZE))\n",
        "  # print(f\"train dataset: {train_dataset}\")\n",
        "\n",
        "  val_dataset = dataset.skip(int(TRAIN_RATIO * DATASET_SIZE)).take(int(VAL_RATIO * DATASET_SIZE))\n",
        "  # print(f\"val dataset: {val_dataset}\")\n",
        "\n",
        "  test_dataset = dataset.skip(int((TRAIN_RATIO + VAL_RATIO) * DATASET_SIZE))\n",
        "  # print(f\"test_dataset: {val_dataset}\")\n",
        "\n",
        "  return train_dataset, val_dataset, test_dataset"
      ],
      "metadata": {
        "id": "MQH0Yj1yEMZ5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset, test_dataset = splits(dataset[0], TRAIN_RATIO, VAL_RATIO, TEST_RATIO)"
      ],
      "metadata": {
        "id": "TM2ChXeRM6KW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IM_SIZE = 224\n",
        "\n",
        "def resizing_rescale(image, label):\n",
        "  return tf.image.resize(image, (IM_SIZE, IM_SIZE)) / 255.0, label"
      ],
      "metadata": {
        "id": "0_ntgO0uNpMc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(resizing_rescale)"
      ],
      "metadata": {
        "id": "9Uman8-7PQni"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = val_dataset.map(resizing_rescale)"
      ],
      "metadata": {
        "id": "ITd7SI7XYyk6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = test_dataset.map(resizing_rescale)"
      ],
      "metadata": {
        "id": "Jp6nymR7YybL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image, label in train_dataset.take(1):\n",
        "  print(image, label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpnH0CofPVbY",
        "outputId": "f8f5950b-2412-48fb-aea5-225ceb9748aa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]], shape=(224, 224, 3), dtype=float32) tf.Tensor(1, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "train_dataset = train_dataset.shuffle(buffer_size=8, reshuffle_each_iteration=True).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "GHQvCmuwPfme"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = val_dataset.shuffle(buffer_size=8, reshuffle_each_iteration=True).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "YVOtfAgmY70b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = test_dataset.shuffle(buffer_size=8, reshuffle_each_iteration=True).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "lF5ki0C1Y7qz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.api._v2.keras import activations\n",
        "model = tf.keras.Sequential([\n",
        "    InputLayer(input_shape = (IM_SIZE, IM_SIZE, 3)),\n",
        "\n",
        "    Conv2D(filters = 6, kernel_size = 5, strides = 1, padding = \"valid\", activation = \"relu\"),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size = 2, strides = 2),\n",
        "\n",
        "    Conv2D(filters = 16, kernel_size = 5, strides = 1, padding = \"valid\", activation = \"relu\"),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size = 2, strides = 2),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(100, activation = \"relu\"),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(10, activation = \"relu\"),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(1, activation = \"sigmoid\")\n",
        "\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6_NQOCoQLHc",
        "outputId": "ca1a7272-0708-4581-c71d-8c94f92840cb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 220, 220, 6)       456       \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 220, 220, 6)       24        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPoolin  (None, 110, 110, 6)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 106, 106, 16)      2416      \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 106, 106, 16)      64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPoolin  (None, 53, 53, 16)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 44944)             0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 100)               4494500   \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 100)               400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 10)                40        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4498921 (17.16 MB)\n",
            "Trainable params: 4498657 (17.16 MB)\n",
            "Non-trainable params: 264 (1.03 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = Adam(learning_rate = 0.08),\n",
        "              loss = BinaryCrossentropy(),\n",
        "              metrics = \"accuracy\")"
      ],
      "metadata": {
        "id": "ZztcjrVjS7dY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset, validation_data = val_dataset, epochs = 10, verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re4FpJkKUP3V",
        "outputId": "861524d8-916f-48b1-fba8-1e942e3f0472"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "603/603 [==============================] - 41s 67ms/step - loss: 0.3873 - accuracy: 0.8388 - val_loss: 0.3351 - val_accuracy: 0.9098\n",
            "Epoch 2/10\n",
            "603/603 [==============================] - 48s 79ms/step - loss: 0.4967 - accuracy: 0.7456 - val_loss: 1.0216 - val_accuracy: 0.4972\n",
            "Epoch 3/10\n",
            "603/603 [==============================] - 47s 78ms/step - loss: 0.4437 - accuracy: 0.8025 - val_loss: 0.6752 - val_accuracy: 0.8369\n",
            "Epoch 4/10\n",
            "603/603 [==============================] - 40s 66ms/step - loss: 0.2606 - accuracy: 0.9059 - val_loss: 0.5065 - val_accuracy: 0.7501\n",
            "Epoch 5/10\n",
            "603/603 [==============================] - 41s 68ms/step - loss: 0.2123 - accuracy: 0.9295 - val_loss: 0.2245 - val_accuracy: 0.9301\n",
            "Epoch 6/10\n",
            "603/603 [==============================] - 47s 78ms/step - loss: 0.1933 - accuracy: 0.9362 - val_loss: 0.1656 - val_accuracy: 0.9479\n",
            "Epoch 7/10\n",
            "603/603 [==============================] - 47s 78ms/step - loss: 0.1821 - accuracy: 0.9418 - val_loss: 0.3077 - val_accuracy: 0.9316\n",
            "Epoch 8/10\n",
            "603/603 [==============================] - 40s 66ms/step - loss: 0.2071 - accuracy: 0.9310 - val_loss: 0.3393 - val_accuracy: 0.9102\n",
            "Epoch 9/10\n",
            "603/603 [==============================] - 58s 96ms/step - loss: 0.1773 - accuracy: 0.9426 - val_loss: 0.6496 - val_accuracy: 0.8911\n",
            "Epoch 10/10\n",
            "603/603 [==============================] - 41s 69ms/step - loss: 0.1701 - accuracy: 0.9444 - val_loss: 0.3075 - val_accuracy: 0.9209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-eIGZdSwYnBO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IoZqcwFTeCYe"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "adntykjheCb1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}